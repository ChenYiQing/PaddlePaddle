{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成测试数据\n",
    "\n",
    "* 10组每组100张 训练数据\n",
    "    * img_i\n",
    "    * 0~99.png\n",
    "    * trainData_i.dt\n",
    "* 1组100张 测试数据\n",
    "    * imgTest\n",
    "    * testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mkdir\n",
    "import os,sys\n",
    "path = \"./testDir\"\n",
    "if os.path.exists(path):\n",
    "    print('dir exist')\n",
    "else:\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import random\n",
    "import os,sys\n",
    "\n",
    "imgW = 28\n",
    "imgH = 28\n",
    "\n",
    "def generalRec():\n",
    "    img = np.ones((imgW,imgH),dtype=np.uint8)\n",
    "    InitX = random.randint(2,imgW-5)\n",
    "    InitY = random.randint(2,imgH-5)\n",
    "    RecW = random.randint(4,imgW-InitX-1)\n",
    "    RecH = random.randint(4,imgH-InitY-1)\n",
    "    cv.rectangle(img,(InitX,InitY),(RecW,RecH),255,1)\n",
    "#     #显示图片\n",
    "#     cv.namedWindow(\"img\")\n",
    "#     cv.imshow(\"img\",img)\n",
    "#     cv.waitKey(0)\n",
    "#     cv.destroyAllWindows()\n",
    "    return img\n",
    "\n",
    "\n",
    "    \n",
    "def generalCircle():\n",
    "    img = np.ones((imgW,imgH),dtype=np.uint8)\n",
    "    InitX = random.randint(5,imgW-5)\n",
    "    InitY = random.randint(5,imgH-5)\n",
    "    minDis = min(5,imgW-InitX,imgH-InitY)\n",
    "    r = random.randint(2,minDis)\n",
    "    cv.circle(img,(InitX,InitY),r,255,1)\n",
    "#     #显示图片\n",
    "#     cv.namedWindow(\"img\")\n",
    "#     cv.imshow(\"img\",img)\n",
    "#     cv.waitKey(0)\n",
    "#     cv.destroyAllWindows()\n",
    "    return img\n",
    "    \n",
    "\n",
    "def GenerateTrainData(genNum,imageDir,dataFilePath):\n",
    "#     trainDataFile = \"./trainData/trainData_\"+str(index)+\".dt\" \n",
    "    with open(dataFilePath,'w') as f:\n",
    "        for i in range(genNum):\n",
    "            randBoo = random.choice([True,False])\n",
    "            if(randBoo):\n",
    "#                 # general rec\n",
    "                genImg = generalRec()\n",
    "                lineStr = imageDir+\"/\"+i.__str__()+\".png,0\\n\"\n",
    "                f.write(lineStr)\n",
    "                cv.imwrite(imageDir+\"/\"+i.__str__()+\".png\",genImg)\n",
    "            else:\n",
    "                # general circle\n",
    "                genImg = generalCircle()\n",
    "                lineStr = imageDir+\"/\"+i.__str__()+\".png,1\\n\"\n",
    "                f.write(lineStr)\n",
    "                cv.imwrite(imageDir+\"/\"+i.__str__()+\".png\",genImg)\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "dataDir = \"./trainData\"\n",
    "if os.path.exists(dataDir):\n",
    "    print('DataDir exist')\n",
    "else:\n",
    "    os.makedirs(dataDir)    \n",
    "\n",
    "for i in range(100):\n",
    "    path = \"./img/img_\"+str(i)\n",
    "    if os.path.exists(path):\n",
    "        print('dir exist')\n",
    "    else:\n",
    "        os.makedirs(path)\n",
    "    dataFilePath = \"./trainData/trainData_\"+str(i)+\".dt\" \n",
    "    GenerateTrainData(100,path,dataFilePath)\n",
    "    \n",
    "testImgPath = \"./img/img_test\"\n",
    "if os.path.exists(testImgPath):\n",
    "    print('DataDir exist')\n",
    "else:\n",
    "    os.makedirs(testImgPath)    \n",
    "testDataFilePath = \"./trainData/testData.dt\" \n",
    "GenerateTrainData(100,testImgPath,testDataFilePath)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据读取验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    line = f.readline()\n",
    "    while line:\n",
    "        strSplit=line.split(',',1)\n",
    "        label_input=int(strSplit[1])\n",
    "        print(label_input)\n",
    "        img = cv.imread(strSplit[0],cv.IMREAD_GRAYSCALE)\n",
    "#         print(img)\n",
    "#         npImg = np.reshape(img,(1,784))\n",
    "#         print(npImg)\n",
    "#         cv.namedWindow(\"img\")\n",
    "#         cv.imshow(\"img\",img)\n",
    "#         cv.waitKey(0)\n",
    "#         cv.destroyAllWindows()\n",
    "        line = f.readline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy\n",
    "\n",
    "def readCustomDataFromFile(trainDataPath):\n",
    "    read_img = np.empty((10,1,28,28),dtype='float32')\n",
    "    read_label = np.empty((10,1),dtype='int64')\n",
    "\n",
    "    with open(trainDataPath) as f:\n",
    "        for i in range(10):\n",
    "            line = f.readline()\n",
    "            if line:\n",
    "                strSplit=line.split(',',1)\n",
    "                img = cv.imread(strSplit[0],cv.IMREAD_GRAYSCALE)\n",
    "                read_img[i][0] = img\n",
    "                read_label[i] = int(strSplit[1])\n",
    "            else:\n",
    "                break\n",
    "    print(read_label)\n",
    "    read_img =read_img/255.0*2.0 - 1.0\n",
    "    return read_img,read_label\n",
    "\n",
    "\n",
    "\n",
    "a,b = readCustomDataFromFile('trainData/trainData_1.dt')\n",
    "\n",
    "# http://www.paddlepaddle.org/documentation/docs/zh/1.2/user_guides/howto/prepare_data/reader_cn.html\n",
    "# images = images / 255.0 * 2.0 - 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.empty((32,1),dtype=float)\n",
    "a[0][0]=123\n",
    "# print(a)\n",
    "\n",
    "b =np.empty((1,1,2,2),dtype=float)\n",
    "c =np.empty((2,2),dtype=float)\n",
    "c[0][0] = 1.1\n",
    "c[0][1] = 2.2\n",
    "c[1][0] = 3.3\n",
    "c[1][1] = 4.4\n",
    "b[0][0] = c\n",
    "print(c)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=numpy.random.random(size=(10, 1, 28, 28)).astype('float32')\n",
    "print(a)\n",
    "b=numpy.random.random(size=(10, 1)).astype('int64')\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Batch 0, Cost 1.098854\n",
      "Test with Epoch 0, avg_cost: 1.2697521448135376, acc: 0.5\n",
      " Batch 1, Cost 0.814255\n",
      "Test with Epoch 1, avg_cost: 1.3725850582122803, acc: 0.5\n",
      " Batch 2, Cost 0.445294\n",
      "Test with Epoch 2, avg_cost: 1.1089836359024048, acc: 0.5\n",
      " Batch 3, Cost 0.352901\n",
      "Test with Epoch 3, avg_cost: 1.065773844718933, acc: 0.5099999904632568\n",
      " Batch 4, Cost 0.219641\n",
      "Test with Epoch 4, avg_cost: 1.1483858823776245, acc: 0.5099999904632568\n",
      " Batch 5, Cost 0.282905\n",
      "Test with Epoch 5, avg_cost: 1.1783555746078491, acc: 0.5099999904632568\n",
      " Batch 6, Cost 0.223437\n",
      "Test with Epoch 6, avg_cost: 1.1972203254699707, acc: 0.5099999904632568\n",
      " Batch 7, Cost 0.134411\n",
      "Test with Epoch 7, avg_cost: 1.14741849899292, acc: 0.5099999904632568\n",
      " Batch 8, Cost 0.101017\n",
      "Test with Epoch 8, avg_cost: 1.0891480445861816, acc: 0.5099999904632568\n",
      " Batch 9, Cost 0.127678\n",
      "Test with Epoch 9, avg_cost: 0.9992412328720093, acc: 0.5399999618530273\n",
      " Batch 10, Cost 0.122820\n",
      "Test with Epoch 10, avg_cost: 0.8755476474761963, acc: 0.5999999642372131\n",
      " Batch 11, Cost 0.142358\n",
      "Test with Epoch 11, avg_cost: 0.7253401279449463, acc: 0.6399999856948853\n",
      " Batch 12, Cost 0.091936\n",
      "Test with Epoch 12, avg_cost: 0.5977859497070312, acc: 0.699999988079071\n",
      " Batch 13, Cost 0.039866\n",
      "Test with Epoch 13, avg_cost: 0.49247676134109497, acc: 0.7699999809265137\n",
      " Batch 14, Cost 0.050147\n",
      "Test with Epoch 14, avg_cost: 0.4129599630832672, acc: 0.7899999618530273\n",
      " Batch 15, Cost 0.042111\n",
      "Test with Epoch 15, avg_cost: 0.3381310999393463, acc: 0.8499999642372131\n",
      " Batch 16, Cost 0.048783\n",
      "Test with Epoch 16, avg_cost: 0.28368979692459106, acc: 0.8899999856948853\n",
      " Batch 17, Cost 0.055731\n",
      "Test with Epoch 17, avg_cost: 0.2486511915922165, acc: 0.8899999856948853\n",
      " Batch 18, Cost 0.028956\n",
      "Test with Epoch 18, avg_cost: 0.22360600531101227, acc: 0.8899999856948853\n",
      " Batch 19, Cost 0.073359\n",
      "Test with Epoch 19, avg_cost: 0.20655585825443268, acc: 0.8899999856948853\n",
      " Batch 20, Cost 0.046970\n",
      "Test with Epoch 20, avg_cost: 0.19069576263427734, acc: 0.8999999761581421\n",
      " Batch 21, Cost 0.029106\n",
      "Test with Epoch 21, avg_cost: 0.1743910014629364, acc: 0.8999999761581421\n",
      " Batch 22, Cost 0.076067\n",
      "Test with Epoch 22, avg_cost: 0.15634770691394806, acc: 0.9099999666213989\n",
      " Batch 23, Cost 0.051221\n",
      "Test with Epoch 23, avg_cost: 0.14119312167167664, acc: 0.9199999570846558\n",
      " Batch 24, Cost 0.071015\n",
      "Test with Epoch 24, avg_cost: 0.11903601139783859, acc: 0.9599999785423279\n",
      " Batch 25, Cost 0.038946\n",
      "Test with Epoch 25, avg_cost: 0.09789911657571793, acc: 0.9599999785423279\n",
      " Batch 26, Cost 0.043303\n",
      "Test with Epoch 26, avg_cost: 0.07705345749855042, acc: 0.9599999785423279\n",
      " Batch 27, Cost 0.033508\n",
      "Test with Epoch 27, avg_cost: 0.06217479705810547, acc: 0.9899999499320984\n",
      " Batch 28, Cost 0.014030\n",
      "Test with Epoch 28, avg_cost: 0.05076434090733528, acc: 0.9899999499320984\n",
      " Batch 29, Cost 0.031893\n",
      "Test with Epoch 29, avg_cost: 0.04221540316939354, acc: 0.9899999499320984\n",
      " Batch 30, Cost 0.023329\n",
      "Test with Epoch 30, avg_cost: 0.03420586138963699, acc: 0.9899999499320984\n",
      " Batch 31, Cost 0.051079\n",
      "Test with Epoch 31, avg_cost: 0.027618322521448135, acc: 0.9899999499320984\n",
      " Batch 32, Cost 0.008867\n",
      "Test with Epoch 32, avg_cost: 0.02360556460916996, acc: 0.9899999499320984\n",
      " Batch 33, Cost 0.028209\n",
      "Test with Epoch 33, avg_cost: 0.021036002784967422, acc: 0.9899999499320984\n",
      " Batch 34, Cost 0.036373\n",
      "Test with Epoch 34, avg_cost: 0.019448602572083473, acc: 0.9899999499320984\n",
      " Batch 35, Cost 0.025391\n",
      "Test with Epoch 35, avg_cost: 0.01822580024600029, acc: 1.0\n",
      " Batch 36, Cost 0.028422\n",
      "Test with Epoch 36, avg_cost: 0.017160993069410324, acc: 1.0\n",
      " Batch 37, Cost 0.019072\n",
      "Test with Epoch 37, avg_cost: 0.01633238047361374, acc: 1.0\n",
      " Batch 38, Cost 0.036806\n",
      "Test with Epoch 38, avg_cost: 0.01571948081254959, acc: 1.0\n",
      " Batch 39, Cost 0.012050\n",
      "Test with Epoch 39, avg_cost: 0.015183484181761742, acc: 1.0\n",
      " Batch 40, Cost 0.015512\n",
      "Test with Epoch 40, avg_cost: 0.015004150569438934, acc: 1.0\n",
      " Batch 41, Cost 0.023576\n",
      "Test with Epoch 41, avg_cost: 0.014886599034070969, acc: 1.0\n",
      " Batch 42, Cost 0.013801\n",
      "Test with Epoch 42, avg_cost: 0.015065131708979607, acc: 1.0\n",
      " Batch 43, Cost 0.017245\n",
      "Test with Epoch 43, avg_cost: 0.01520046778023243, acc: 1.0\n",
      " Batch 44, Cost 0.010107\n",
      "Test with Epoch 44, avg_cost: 0.015372708439826965, acc: 0.9899999499320984\n",
      " Batch 45, Cost 0.007532\n",
      "Test with Epoch 45, avg_cost: 0.015550393611192703, acc: 0.9899999499320984\n",
      " Batch 46, Cost 0.007602\n",
      "Test with Epoch 46, avg_cost: 0.01580224558711052, acc: 0.9899999499320984\n",
      " Batch 47, Cost 0.006226\n",
      "Test with Epoch 47, avg_cost: 0.0160933006554842, acc: 0.9899999499320984\n",
      " Batch 48, Cost 0.016167\n",
      "Test with Epoch 48, avg_cost: 0.015609879046678543, acc: 0.9899999499320984\n",
      " Batch 49, Cost 0.002080\n",
      "Test with Epoch 49, avg_cost: 0.015225471928715706, acc: 0.9899999499320984\n",
      " Batch 50, Cost 0.007296\n",
      "Test with Epoch 50, avg_cost: 0.015034678392112255, acc: 0.9899999499320984\n",
      " Batch 51, Cost 0.015330\n",
      "Test with Epoch 51, avg_cost: 0.014719190075993538, acc: 0.9899999499320984\n",
      " Batch 52, Cost 0.004705\n",
      "Test with Epoch 52, avg_cost: 0.013903954066336155, acc: 1.0\n",
      " Batch 53, Cost 0.025949\n",
      "Test with Epoch 53, avg_cost: 0.01283925399184227, acc: 1.0\n",
      " Batch 54, Cost 0.006292\n",
      "Test with Epoch 54, avg_cost: 0.011910771019756794, acc: 1.0\n",
      " Batch 55, Cost 0.011234\n",
      "Test with Epoch 55, avg_cost: 0.011047973297536373, acc: 1.0\n",
      " Batch 56, Cost 0.004738\n",
      "Test with Epoch 56, avg_cost: 0.010392768308520317, acc: 1.0\n",
      " Batch 57, Cost 0.016576\n",
      "Test with Epoch 57, avg_cost: 0.009872052818536758, acc: 1.0\n",
      " Batch 58, Cost 0.013467\n",
      "Test with Epoch 58, avg_cost: 0.009456107392907143, acc: 1.0\n",
      " Batch 59, Cost 0.018297\n",
      "Test with Epoch 59, avg_cost: 0.008552349172532558, acc: 1.0\n",
      " Batch 60, Cost 0.004426\n",
      "Test with Epoch 60, avg_cost: 0.008024951443076134, acc: 1.0\n",
      " Batch 61, Cost 0.011666\n",
      "Test with Epoch 61, avg_cost: 0.007422359194606543, acc: 1.0\n",
      " Batch 62, Cost 0.008065\n",
      "Test with Epoch 62, avg_cost: 0.007020227611064911, acc: 1.0\n",
      " Batch 63, Cost 0.004643\n",
      "Test with Epoch 63, avg_cost: 0.006696120835840702, acc: 1.0\n",
      " Batch 64, Cost 0.009549\n",
      "Test with Epoch 64, avg_cost: 0.0064885616302490234, acc: 1.0\n",
      " Batch 65, Cost 0.010558\n",
      "Test with Epoch 65, avg_cost: 0.006275190971791744, acc: 1.0\n",
      " Batch 66, Cost 0.003784\n",
      "Test with Epoch 66, avg_cost: 0.006164612248539925, acc: 1.0\n",
      " Batch 67, Cost 0.018694\n",
      "Test with Epoch 67, avg_cost: 0.005930694751441479, acc: 1.0\n",
      " Batch 68, Cost 0.011206\n",
      "Test with Epoch 68, avg_cost: 0.005691098049283028, acc: 1.0\n",
      " Batch 69, Cost 0.003818\n",
      "Test with Epoch 69, avg_cost: 0.005453972611576319, acc: 1.0\n",
      " Batch 70, Cost 0.007267\n",
      "Test with Epoch 70, avg_cost: 0.00529290409758687, acc: 1.0\n",
      " Batch 71, Cost 0.006960\n",
      "Test with Epoch 71, avg_cost: 0.0051798331551253796, acc: 1.0\n",
      " Batch 72, Cost 0.008471\n",
      "Test with Epoch 72, avg_cost: 0.00502057047560811, acc: 1.0\n",
      " Batch 73, Cost 0.005098\n",
      "Test with Epoch 73, avg_cost: 0.004894630052149296, acc: 1.0\n",
      " Batch 74, Cost 0.005196\n",
      "Test with Epoch 74, avg_cost: 0.00482032960280776, acc: 1.0\n",
      " Batch 75, Cost 0.005752\n",
      "Test with Epoch 75, avg_cost: 0.00468453811481595, acc: 1.0\n",
      " Batch 76, Cost 0.003484\n",
      "Test with Epoch 76, avg_cost: 0.004590241704136133, acc: 1.0\n",
      " Batch 77, Cost 0.002647\n",
      "Test with Epoch 77, avg_cost: 0.00451284646987915, acc: 1.0\n",
      " Batch 78, Cost 0.006532\n",
      "Test with Epoch 78, avg_cost: 0.004458032548427582, acc: 1.0\n",
      " Batch 79, Cost 0.006004\n",
      "Test with Epoch 79, avg_cost: 0.00441155256703496, acc: 1.0\n",
      " Batch 80, Cost 0.003479\n",
      "Test with Epoch 80, avg_cost: 0.004363889805972576, acc: 1.0\n",
      " Batch 81, Cost 0.009022\n",
      "Test with Epoch 81, avg_cost: 0.004284881055355072, acc: 1.0\n",
      " Batch 82, Cost 0.003667\n",
      "Test with Epoch 82, avg_cost: 0.004190456587821245, acc: 1.0\n",
      " Batch 83, Cost 0.005689\n",
      "Test with Epoch 83, avg_cost: 0.004124728497117758, acc: 1.0\n",
      " Batch 84, Cost 0.007799\n",
      "Test with Epoch 84, avg_cost: 0.004041945096105337, acc: 1.0\n",
      " Batch 85, Cost 0.007018\n",
      "Test with Epoch 85, avg_cost: 0.0039266012609004974, acc: 1.0\n",
      " Batch 86, Cost 0.008968\n",
      "Test with Epoch 86, avg_cost: 0.0038734530098736286, acc: 1.0\n",
      " Batch 87, Cost 0.010617\n",
      "Test with Epoch 87, avg_cost: 0.003819274716079235, acc: 1.0\n",
      " Batch 88, Cost 0.007193\n",
      "Test with Epoch 88, avg_cost: 0.003793838433921337, acc: 1.0\n",
      " Batch 89, Cost 0.002967\n",
      "Test with Epoch 89, avg_cost: 0.003762478707358241, acc: 1.0\n",
      " Batch 90, Cost 0.011301\n",
      "Test with Epoch 90, avg_cost: 0.0036480133421719074, acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Batch 91, Cost 0.001375\n",
      "Test with Epoch 91, avg_cost: 0.003565774532034993, acc: 1.0\n",
      " Batch 92, Cost 0.007615\n",
      "Test with Epoch 92, avg_cost: 0.003436906961724162, acc: 1.0\n",
      " Batch 93, Cost 0.003050\n",
      "Test with Epoch 93, avg_cost: 0.003395586274564266, acc: 1.0\n",
      " Batch 94, Cost 0.007357\n",
      "Test with Epoch 94, avg_cost: 0.0033274011220782995, acc: 1.0\n",
      " Batch 95, Cost 0.006532\n",
      "Test with Epoch 95, avg_cost: 0.0032573461066931486, acc: 1.0\n",
      " Batch 96, Cost 0.003286\n",
      "Test with Epoch 96, avg_cost: 0.0031823900062590837, acc: 1.0\n",
      " Batch 97, Cost 0.003916\n",
      "Test with Epoch 97, avg_cost: 0.003177330829203129, acc: 1.0\n",
      " Batch 98, Cost 0.002510\n",
      "Test with Epoch 98, avg_cost: 0.003142080269753933, acc: 1.0\n",
      " Batch 99, Cost 0.003405\n",
      "Test with Epoch 99, avg_cost: 0.003086242126300931, acc: 1.0\n",
      "train finsh~~~\n",
      "recognize_digits_convolutional_neural_network.inference.model\n",
      "None\n",
      "[array([[2.2562226e-05, 9.9997747e-01]], dtype=float32)]\n",
      "[[[0 1]]]\n",
      "Inference result of image/circle.png is: 1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy\n",
    "import paddle\n",
    "import paddle.fluid as fluid\n",
    "import cv2 as cv\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "PASS_NUM = 5\n",
    "\n",
    "\n",
    "def readCustomDataFromFile(trainDataPath):\n",
    "    read_img = numpy.empty((100,1,28,28),dtype='float32')\n",
    "    read_label = numpy.empty((100,1),dtype='int64')\n",
    "\n",
    "    with open(trainDataPath) as f:\n",
    "        for i in range(100):\n",
    "            line = f.readline()\n",
    "            if line:\n",
    "                strSplit=line.split(',',1)\n",
    "                img = cv.imread(strSplit[0],cv.IMREAD_GRAYSCALE)\n",
    "                read_img[i][0] = img\n",
    "                read_label[i] = int(strSplit[1])\n",
    "            else:\n",
    "                break\n",
    "    read_img =read_img/255.0*2.0 - 1.0\n",
    "    return read_img,read_label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def loss_net(hidden, label):\n",
    "    prediction = fluid.layers.fc(input=hidden, size=2, act='softmax')\n",
    "    loss = fluid.layers.cross_entropy(input=prediction, label=label)\n",
    "    avg_loss = fluid.layers.mean(loss)\n",
    "    acc = fluid.layers.accuracy(input=prediction, label=label)\n",
    "    return prediction, avg_loss, acc\n",
    "\n",
    "\n",
    "def multilayer_perceptron(img, label):\n",
    "    img = fluid.layers.fc(input=img, size=200, act='tanh')\n",
    "    hidden = fluid.layers.fc(input=img, size=200, act='tanh')\n",
    "    return loss_net(hidden, label)\n",
    "\n",
    "\n",
    "def softmax_regression(img, label):\n",
    "    return loss_net(img, label)\n",
    "\n",
    "\n",
    "def convolutional_neural_network(img, label):\n",
    "    conv_pool_1 = fluid.nets.simple_img_conv_pool(\n",
    "        input=img,\n",
    "        filter_size=5,\n",
    "        num_filters=20,\n",
    "        pool_size=2,\n",
    "        pool_stride=2,\n",
    "        act=\"relu\")\n",
    "    conv_pool_1 = fluid.layers.batch_norm(conv_pool_1)\n",
    "    conv_pool_2 = fluid.nets.simple_img_conv_pool(\n",
    "        input=conv_pool_1,\n",
    "        filter_size=5,\n",
    "        num_filters=50,\n",
    "        pool_size=2,\n",
    "        pool_stride=2,\n",
    "        act=\"relu\")\n",
    "    return loss_net(conv_pool_2, label)\n",
    "\n",
    "\n",
    "def train(nn_type,\n",
    "          use_cuda,\n",
    "          save_dirname=None,\n",
    "          model_filename=None,\n",
    "          params_filename=None):\n",
    "    if use_cuda and not fluid.core.is_compiled_with_cuda():\n",
    "        return\n",
    "\n",
    "    img = fluid.layers.data(name='img', shape=[1, 28, 28], dtype='float32')\n",
    "    label = fluid.layers.data(name='label', shape=[1], dtype='int64')\n",
    "\n",
    "    if nn_type == 'softmax_regression':\n",
    "        net_conf = softmax_regression\n",
    "    elif nn_type == 'multilayer_perceptron':\n",
    "        net_conf = multilayer_perceptron\n",
    "    else:\n",
    "        net_conf = convolutional_neural_network\n",
    "\n",
    "    prediction, avg_loss, acc = net_conf(img, label)\n",
    "\n",
    "    test_program = fluid.default_main_program().clone(for_test=True)\n",
    "\n",
    "    optimizer = fluid.optimizer.Adam(learning_rate=0.001)\n",
    "    optimizer.minimize(avg_loss)\n",
    "\n",
    "    \n",
    "    def train_test(train_test_program):\n",
    "        acc_set = []\n",
    "        avg_loss_set = []\n",
    "        readImg,readLabel = readCustomDataFromFile('trainData/testData.dt')        \n",
    "        acc_np, avg_loss_np = exe.run(\n",
    "            test_program,\n",
    "            feed={\n",
    "                \"img\": readImg,\n",
    "                \"label\": readLabel\n",
    "            },\n",
    "            fetch_list=[acc, avg_loss])\n",
    "        acc_set.append(float(acc_np))\n",
    "        avg_loss_set.append(float(avg_loss_np))\n",
    "        # get test acc and loss\n",
    "        acc_val_mean = numpy.array(acc_set).mean()\n",
    "        avg_loss_val_mean = numpy.array(avg_loss_set).mean()\n",
    "        return avg_loss_val_mean, acc_val_mean\n",
    "\n",
    "    place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()\n",
    "\n",
    "    exe = fluid.Executor(place)\n",
    "\n",
    "    train_reader = paddle.batch(\n",
    "        paddle.reader.shuffle(paddle.dataset.mnist.train(), buf_size=500),\n",
    "        batch_size=BATCH_SIZE)\n",
    "    test_reader = paddle.batch(\n",
    "        paddle.dataset.mnist.test(), batch_size=BATCH_SIZE)\n",
    "    feeder = fluid.DataFeeder(feed_list=[img, label], place=place)\n",
    "\n",
    "    exe.run(fluid.default_startup_program())\n",
    "    main_program = fluid.default_main_program()\n",
    "    epochs = [epoch_id for epoch_id in range(PASS_NUM)]\n",
    "\n",
    "    lists = []\n",
    "\n",
    "#     for epoch_id in epochs:\n",
    "    for epoch_id in range(100):\n",
    "        trainDataFilePath = 'trainData/trainData_'+str(epoch_id)+'.dt'\n",
    "        readImg,readLabel = readCustomDataFromFile(trainDataFilePath)\n",
    "        metrics = exe.run(\n",
    "            main_program,\n",
    "            feed={\n",
    "                \"img\": readImg,\n",
    "                \"label\": readLabel\n",
    "            },\n",
    "            fetch_list=[avg_loss, acc])\n",
    "\n",
    "        print(\" Batch %d, Cost %f\" % (epoch_id,metrics[0]))\n",
    "\n",
    "            \n",
    "        # test for epoch\n",
    "        avg_loss_val, acc_val = train_test(test_program)\n",
    "        print(\"Test with Epoch %d, avg_cost: %s, acc: %s\" %\n",
    "              (epoch_id, avg_loss_val, acc_val))\n",
    "        lists.append((epoch_id, avg_loss_val, acc_val))\n",
    "        \n",
    "        if save_dirname is not None:\n",
    "            fluid.io.save_inference_model(\n",
    "                save_dirname, [\"img\"], [prediction],\n",
    "                exe,\n",
    "                model_filename=model_filename,\n",
    "                params_filename=params_filename)\n",
    "\n",
    "    # find the best pass\n",
    "#     best = sorted(lists, key=lambda list: float(list[1]))[0]\n",
    "#     print('Best pass is %s, testing Avgcost is %s' % (best[0], best[1]))\n",
    "#     print('The classification accuracy is %.2f%%' % (float(best[2]) * 100))\n",
    "\n",
    "\n",
    "def infer(use_cuda,\n",
    "          save_dirname=None,\n",
    "          model_filename=None,\n",
    "          params_filename=None):\n",
    "    if save_dirname is None:\n",
    "        return\n",
    "\n",
    "    place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()\n",
    "    exe = fluid.Executor(place)\n",
    "\n",
    "#     def load_image(file):\n",
    "#         im = Image.open(file).convert('L')\n",
    "#         im = im.resize((28, 28), Image.ANTIALIAS)\n",
    "#         im = numpy.array(im).reshape(1, 1, 28, 28).astype(numpy.float32)\n",
    "#         im = im / 255.0 * 2.0 - 1.0\n",
    "#         return im\n",
    "\n",
    "    def load_image(file):\n",
    "        read_img = numpy.empty((1,1,28,28),dtype='float32')\n",
    "        im = cv.imread(file,cv.IMREAD_GRAYSCALE)\n",
    "        read_img[0][0] = im\n",
    "        read_img =read_img/255.0*2.0 - 1.0\n",
    "        return read_img\n",
    "    \n",
    "#     tensor_img = load_image('./image/rec.png')\n",
    "    tensor_img = load_image('./image/circle.png')\n",
    "    \n",
    "\n",
    "    inference_scope = fluid.core.Scope()\n",
    "    with fluid.scope_guard(inference_scope):\n",
    "        # Use fluid.io.load_inference_model to obtain the inference program desc,\n",
    "        # the feed_target_names (the names of variables that will be feeded\n",
    "        # data using feed operators), and the fetch_targets (variables that\n",
    "        # we want to obtain data from using fetch operators).\n",
    "        [inference_program, feed_target_names,\n",
    "         fetch_targets] = fluid.io.load_inference_model(\n",
    "             save_dirname, exe, model_filename, params_filename)\n",
    "\n",
    "        # Construct feed as a dictionary of {feed_target_name: feed_target_data}\n",
    "        # and results will contain a list of data corresponding to fetch_targets.\n",
    "        results = exe.run(\n",
    "            inference_program,\n",
    "            feed={feed_target_names[0]: tensor_img},\n",
    "            fetch_list=fetch_targets)\n",
    "        print(results)\n",
    "        lab = numpy.argsort(results)\n",
    "        print(lab)\n",
    "        print(\"Inference result of image/circle.png is: %d\" % lab[0][0][-1])\n",
    "\n",
    "\n",
    "def main(use_cuda, nn_type):\n",
    "    model_filename = None\n",
    "    params_filename = None\n",
    "    save_dirname = \"recognize_digits_\" + nn_type + \".inference.model\"\n",
    "\n",
    "    # call train() with is_local argument to run distributed train\n",
    "    train(\n",
    "        nn_type=nn_type,\n",
    "        use_cuda=use_cuda,\n",
    "        save_dirname=save_dirname,\n",
    "        model_filename=model_filename,\n",
    "        params_filename=params_filename)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print('train finsh~~~')\n",
    "    print(save_dirname)\n",
    "    print(model_filename)\n",
    "    infer(\n",
    "        use_cuda=use_cuda,\n",
    "        save_dirname=save_dirname,\n",
    "        model_filename=model_filename,\n",
    "        params_filename=params_filename)\n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    use_cuda = True\n",
    "    #predict = 'softmax_regression' # uncomment for Softmax\n",
    "    #predict = 'multilayer_perceptron' # uncomment for MLP\n",
    "    predict = 'convolutional_neural_network'  # uncomment for LeNet5\n",
    "    main(use_cuda=use_cuda, nn_type=predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 记录\n",
    "* 运行第二次的时候会报错\n",
    "    * Restart Kernel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
